llm_model=meta-llama/Meta-Llama-3.1-8B-Instruct
# tiiuae/Falcon3-7B-Instruct
# deepseek-ai/DeepSeek-R1-Distill-Llama-8B
#Qwen/Qwen3-8B
#Qwen/Qwen2.5-7B-Instruct
# meta-llama/Meta-Llama-3.1-8B-Instruct 
#meta-llama/Llama-3.2-3B-Instruct
# Meta-Llama-3.1-8B-Instruct-Q8_0.gguf
llm_type=llama_hf #llama_cpp_quant #llama_hf #deepseek #falcon #qwen3 #qwen2_5 #llama3_3b

llm_cpp_quant=Meta-Llama-3.1-8B-Instruct-Q8_0.gguf
llm_deepseek=deepseek-ai/DeepSeek-R1-Distill-Llama-8B
llm_falcon =tiiuae/Falcon3-7B-Instruct
llm_qwen3=Qwen/Qwen3-8B
llm_qwen2_5=Qwen/Qwen2.5-7B-Instruct
llama32_3b=meta-llama/Llama-3.2-3B-Instruct
llama_cpp_model_path=


quantized_model=True



# llm_model=Qwen/Qwen2.5-3B-Instruct
llm_assistant_token="<|eot_id|>assistant\n\n"
# <|end_header_id|> #llama_32_3b
#<|im_start|>assistant #qwen2_5
#</think>      #qwen3
# "<|eot_id|>assistant\n\n"    #llama    #deepseek
# <|assistant|>     #falcon



# avsolatorio/GIST-small-Embedding-v0
embedding_model=avsolatorio/GIST-small-Embedding-v0
# intfloat/e5-base-v2 
# BAAI/bge-small-en-v1.5
trust_remote_code=True
force_cpu=False
force_reload_documents=True

provenance_method=rerank
provenance_similarity_llm=sentence-transformers/distiluse-base-multilingual-cased-v2
provenance_include_query=False
provenance_llm_prompt="Instruction: You are a provenance auditor that needs to exactly determine how much an answer given to a user question was based on a given input document, knowing that more than just that one document were considered. Documents may be fully used verbatim, partially used or even translated. You need to give a score indicating how much a source document was used in creating the answer given to a user query, this score must be 0 = source document is not used at all, 1 = barely used, 2 = moderately used, 3 = mostly used, 4 = almost fully used and 5 = full text included in answer. You are forced to always answer only with the score from 0 to 5, don't explain yourself or add more text than just the score.

The user's query is:

{query}

The answer given is to this user query is:

{answer}

The source document that you need to score is the following:

{context}"

data_directory=
sharepoint_data_directory=

# data_directory=
# sharepoint_data_directory=
file_types=pdf,json,docx,pptx,xslx,csv,xml,txt
json_schema="."
json_text_content=False
xml_xpath="//"

max_document_limit=10
neo4j_location='URL_to_neo4j_server'
vector_store=milvus
vector_store_uri='data.db'
vector_store_collection=ragmeup_documents
vector_store_sparse_uri=bm25_db.pickle
vector_store_initial_load=True
vector_store_k=10
document_chunks_pickle=rag_chunks.pickle
file_upload_using_llm=True
dynamic_neo4j_schema=False
rerank=True
rerank_k=5
rerank_model=cross-encoder/ms-marco-MiniLM-L-12-v2
#cross-encoder/ms-marco-MiniLM-L-6-v2
#cross-encoder/ms-marco-MiniLM-L-12-v2
# BAAI/bge-reranker-v2-m3
normalize_provenance=True
temperature=0.2
repetition_penalty=1.1
max_new_tokens=1000

rag_instruction="Instruction: You are a digital librarian specializing in answering questions about clients. The user may ask about a specific client (e.g., VIPCOM, Lasalle). Do not mix up clients. Only include information from documents clearly related to the client mentioned in the user query. Be concise and always cite the source document you used to formulate the answer:\n\n{context}"
rag_question_initial="{question}"

rag_question_followup="You are a helpful assistant answering questions strictly based on the retrieved documents. Use the full chat history to understand which client the user is asking about. Do not mix up information from different clients. You will get the document context and chat history:\n\nContext:\n{context}\n\nChat History:\n{history}\n\nAnswer the user question strictly using the documents about the same client."

rag_fetch_new_instruction="Instruction: You are a digital librarian with a database that contains relevant documents for user queries. Users want to ask questions based on those documents and ask questions that either need you to fetch new documents from the database or that are a followup question on previously obtained documents. You need to decide whether you are going to fetch new documents or whether the user is asking a follow-up question but you don't get to see the actual documents the user potentially is looking at.\nShould new documents be fetched from the database based on this user query? Answer with yes or no."
rag_fetch_new_question="The user question is the following: \"{question}\"\n"

use_rewrite_loop=False #original True
rewrite_query_instruction="You have to answer a user question based on documents retrieved from a document database. It is your task to decide whether or not the documents contain the answer to the user's query. You can always only answer with exactly yes or no. The documents that are currently fetched from the database are:

{context}"
rewrite_query_question="The user's question is:

{question}"
rewrite_query_prompt="You are given a user query that should be answered by looking up documents that from a document store using a distance based similarity measure. The documents fetched from the document store were found to be irrelevant to answer the question. Rewrite the following question into an alternative that increases the likelihood of finding relevant documents from the database. You may only answer with the exact rephrasing. The original question is: {question}"

use_re2=False
re2_prompt="Read the question again: "

# DeepSeek-aware variants (inline prompt style)
rag_instruction_deepseek=You are a helpful assistant. Answer only using the context below. Do not invent. Cite the source.\n\n{context}

rag_question_initial_deepseek=<think>\nQuestion: {question}

rag_question_followup_deepseek=<think>\nContext:\n{context}\n\nHistory:\n{history}\n\nQuestion: {question}

rag_fetch_new_instruction_deepseek="You are a digital librarian with a database that contains relevant documents for user queries. Users want to ask questions based on those documents and ask questions that either need you to fetch new documents from the database or that are a followup question on previously obtained documents. You need to decide whether you are going to fetch new documents or whether the user is asking a follow-up question but you don't get to see the actual documents the user potentially is looking at.\nShould new documents be fetched from the database based on this user query? Answer with yes or no."
rag_fetch_new_question_deepseek="The user question is the following: \"{question}\"\n"

splitter='RecursiveCharacterTextSplitter'
chunk_size=512
chunk_overlap=20
breakpoint_threshold_type=percentile
breakpoint_threshold_amount=None
number_of_chunks=None

use_openai=False
openai_model_name='gpt-3.5-turbo'
use_gemini=True
gemini_model_name='gemini-2.0-flash'
GOOGLE_API_KEY=
use_azure=False
use_ollama=False
ollama_model='llama3.1'
# HUGGINGFACE_HUB_TOKEN=
HUGGINGFACE_HUB_TOKEN= 
ragas_judge_model=Qwen/Qwen3-0.6B


ragas_dataset=data/ragas_eval_output
ragas_sample_size=6
ragas_qa_pairs=3
ragas_timeout=12000
ragas_max_workers=1

ragas_question_instruction="You direct another LLM with questions. Given the following documents, write exactly one question that refers to only one company or entity mentioned in the documents. Do not include multiple clients in the same question.

Only output the question text, nothing else.

{context}"

ragas_question_query="Generate a single, clear, answerable question based on only one client from the documents above. Never include more than one client in the question. Do not explain."

ragas_answer_instruction="You are a digital librarian and must answer the following question based only on the documents below.

{context}"

ragas_answer_query="Answer the question below using only the content of the documents. Do not include anything else:

{question}"


